1.CODE:
-------------------------------------------------------------------------------------------------------------------
!pip install ultralytics roboflow
# !pip install roboflow
from roboflow import Roboflow
rf = Roboflow(api_key="3Z64ukhZtkaSGicOfJ9g")
project = rf.workspace("microsoft").project("coco")
version = project.version(34)
dataset = version.download("yolov8")

import os
import random
from PIL import Image
import matplotlib.pyplot as plt

# Set the path to the images directory
image_dir = "/kaggle/working/COCO-Dataset-34/train/images"  # Update with your directory path

# Get a list of all image files
image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]

# Ensure there are enough images for random sampling, otherwise sample all
sample_size = min(5, len(image_files))  # Sample up to 5 images, or fewer if there aren't enough

# Display the random images
random_images = random.sample(image_files, sample_size)

# Display images
image_paths = [os.path.join(image_dir, img) for img in random_images]
images = [Image.open(img_path) for img_path in image_paths]

# Show the images
fig, axes = plt.subplots(1, sample_size, figsize=(15, 5))
for ax, img, img_path in zip(axes, images, image_paths):
    ax.imshow(img)
    ax.set_title(os.path.basename(img_path))
    ax.axis('off')

plt.show()


from ultralytics import YOLO

# Initialize the YOLO model
model = YOLO("yolov8n.pt")  # 'yolov8n.pt' is the smallest YOLOv8 model; consider 'yolov8s.pt' for larger capacity.

# Train the model with specified parameters
results = model.train(
    data="/kaggle/working/COCO-Dataset-34/data.yaml",  # Adjust path to your data.yaml file
    epochs=200,                      # Number of epochs
    imgsz=640,                      # Image size (e.g., 640x640)
    batch=16,                       # Batch size; adjust according to available GPU memory
    project="yolov8-kaggle-training",  # Directory name to store results
    name="Object_detection",    # Experiment name
    cache=True                      # Cache images in memory for faster training
)

import matplotlib.pyplot as plt
import os
import matplotlib.image as mpimg

# Define the directory where the plots are saved
results_dir = "/kaggle/working/yolov8-kaggle-training/Object_detection3"

# List of plot files to display
plot_files = [
    "results.png",                # Overall training results plot
    "confusion_matrix.png",       # Confusion matrix plot
    "F1_curve.png", # Precision-recall curve plot
    "P_curve.png",              # Loss plot
    "PR_curve.png",             # Train loss plot
]

# Iterate over each plot file and display it using matplotlib
for plot_file in plot_files:
    plot_path = os.path.join(results_dir, plot_file)
    
    # Check if the plot file exists
    if os.path.exists(plot_path):
        # Load the plot image
        img = mpimg.imread(plot_path)
        
        # Display the image
        plt.figure(figsize=(10, 6))
        plt.imshow(img)
        plt.axis('off')  # Hide axis
        plt.title(f"Displaying: {plot_file}")
        plt.show()
    else:
        print(f"{plot_file} not found in {results_dir}")

----------------------------------------------------------------------------------------------------------------------------


2. OBJECT CODE :
----------------------------------------------------------------------------------------------------------------------------
import streamlit as st
from ultralytics import YOLO
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import pyttsx3
import tempfile

# Set Streamlit page configuration first to avoid StreamlitSetPageConfigMustBeFirstCommandError
st.set_page_config(page_title="Image-Centric Indoor Navigation Solution for Visually Impaired People", page_icon="üñºÔ∏è")

# Load YOLOv8 model using ultralytics
@st.cache_resource
def load_model():
    return YOLO('yolov8s.pt')  # Replace 'yolov8s.pt' with other variants if needed

model = load_model()

def detect_objects(image):
    """Detect objects in an image using YOLOv8."""
    results = model(image)  # Using YOLOv8's detection method
    return results

def analyze_position(bbox, image_width, image_height):
    """Analyze the position of the object in the image."""
    x_min, y_min, x_max, y_max = bbox
    x_center = (x_min + x_max) / 2
    y_center = (y_min + y_max) / 2

    # Determine horizontal position
    if x_center < image_width / 3:
        horizontal = "Left"
    elif x_center > 2 * image_width / 3:
        horizontal = "Right"
    else:
        horizontal = "Center"

    # Determine vertical position
    if y_center < image_height / 3:
        vertical = "Top"
    elif y_center > 2 * image_height / 3:
        vertical = "Bottom"
    else:
        vertical = "Center"

    # Determine corner positions (Optional)
    if x_center < image_width / 3 and y_center < image_height / 3:
        corner = "Top-Left"
    elif x_center > 2 * image_width / 3 and y_center < image_height / 3:
        corner = "Top-Right"
    elif x_center < image_width / 3 and y_center > 2 * image_height / 3:
        corner = "Bottom-Left"
    elif x_center > 2 * image_width / 3 and y_center > 2 * image_height / 3:
        corner = "Bottom-Right"
    else:
        corner = None

    position = f"{vertical}-{horizontal}"
    if corner:
        position = corner  # Override with corner if applicable

    return position

def generate_speech(all_text):
    """Generate a single speech audio file for all detected objects."""
    engine = pyttsx3.init()  # Initialize the TTS engine
    # Set properties (optional)
    engine.setProperty('rate', 150)  # Speed of speech
    engine.setProperty('volume', 1)  # Volume (0.0 to 1.0)
    
    # Create a temporary file to save the audio
    with tempfile.NamedTemporaryFile(delete=False) as audio_file:
        audio_file_path = audio_file.name + ".mp3"
        engine.save_to_file(all_text, audio_file_path)  # Save to temporary file
        engine.runAndWait()  # Run the engine to generate speech
    return audio_file_path

# Streamlit UI
def home_page():
    st.title("Image-Centric Indoor Navigation Solution for Visually Impaired People")
    st.write("This app detects objects in images, analyzes their positions (left, right, top, bottom, center), and estimates their distance. The results are shown with bounding boxes around detected objects and audio output describing the detection.")
    st.write("Use the sidebar to upload an image and start detection.")

# Object Detection Page
def detection_page():
    st.title("Object Detection with Position Analysis and Voice Output")

    # Upload image
    uploaded_file = st.file_uploader("Upload an image", type=["png", "jpg", "jpeg", "jfif"])

    if uploaded_file:
        image = Image.open(uploaded_file)
        st.image(image, caption="Uploaded Image", use_column_width=True)

        # Detect objects
        with st.spinner("Detecting objects..."):
            results = detect_objects(image)
        
        # Extract image dimensions
        image_width, image_height = image.size

        # Draw bounding boxes and determine positions
        detections = results[0].boxes  # Access the detection boxes from the results
        draw = ImageDraw.Draw(image)
        all_text = ""  # Initialize variable to collect all speech text

        st.write("### Detected Objects:")

        for det in detections:
            x_min, y_min, x_max, y_max = det.xyxy[0]  # Get the bounding box in xyxy format
            confidence = det.conf[0]  # Get the confidence score
            class_id = int(det.cls[0])  # Get the class ID
            label = model.names[class_id]  # Get the label from the class ID (from model.names)

            bbox = [x_min, y_min, x_max, y_max]
            position = analyze_position(bbox, image_width, image_height)

            # Estimate distance using bounding box area (simple heuristic)
            width = x_max - x_min
            height = y_max - y_min
            area = width * height
            distance = max(1, 1000 / np.sqrt(area))  # Example: Larger objects are closer

            # Prepare speech text for the object
            speech_text = f"Detected a {label} at position {position}. Estimated distance: {distance:.2f} cm.\n"
            all_text += speech_text  # Append each object's speech text

            # Print object details in the app
            st.write(f"**{label}** - Confidence: {confidence:.2f}, Position: {position}, Approximate Distance: {distance:.2f} cm")

            # Adjust bounding box thickness and color based on object type
            if label == "person":
                box_thickness = 8  # Thicker box for person
                box_color = "blue"
            elif label == "car":
                box_thickness = 8  # Slightly thinner box for car
                box_color = "green"
            else:
                box_thickness = 8  # Normal box for other objects
                box_color = "red"

            font = ImageFont.load_default()  # Default font, you can replace with a custom one

            # Draw bounding box with custom thickness and label in clear medium font
            draw.rectangle([x_min, y_min, x_max, y_max], outline=box_color, width=box_thickness)
            draw.text((x_min, y_min), f"{label} ({confidence*100:.1f}%)", fill=box_color, font=font)

        # Generate and play the single audio file for all objects
        audio_path = generate_speech(all_text)
        
        # Play the audio of the speech
        st.audio(audio_path, format="audio/mp3")

        # Display image with bounding boxes
        st.image(image, caption="Detected Objects with Bounding Boxes", use_column_width=True)

# Streamlit Sidebar for Navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio("Go to", ["Home", "Object Detection"])

if page == "Home":
    home_page()
else:
    detection_page()

----------------------------------------------------------------------------------------------------------------------------
                                                    THE END
---------------------------------------------------------------------------------------------------------------------------
